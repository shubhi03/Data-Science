Logistic Regression-Used to estimate discreet values(Binary Values like 0/1,Yes/No,True/False) based on independent variable

#------------------Univariate Analysis--------------------
import pandas  as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df_base = pd.read_csv('/home/azad/Data_Science_Courses/01_DataScience-20190528T065230Z-001/01_DataScience/Datasets/Log-Reg-Case-Study.csv')

data = df_base.copy()
arr = df_base.values
X = data.ix[:,(1,10,11,13)] 
y = data.ix[:,16]

from sklearn.linear_model import LogisticRegression

regr = LogisticRegression()
regr.fit(X, y)
pred = regr.predict(X)

from sklearn.metrics import confusion_matrix

cm_df = pd.DataFrame(confusion_matrix(y, pred).T, index=regr.classes_,columns=regr.classes_)
cm_df.index.name = 'Predicted'
cm_df.columns.name = 'True'
cm_df

model_data = df_base.sample(frac=0.7)

model_data = df_base.sample(frac=0.7,random_state=1234)

test_data = df_base.loc[~df_base.index.isin(model_data.index), :]

df = model_data
#------------------Univariate Analysis--------------------
#----------Capping(Handling the outliers)-----------------



print(df.Age.describe())
print(df.Age.quantile(q=0.995))
print(df.Age.quantile(q=0.997))
print(df.Age.quantile(q=0.999))
data = df
print(min(df.Age))
print(max(df.Age))
df.loc[df['Age']>75,'Age'] = 75
print(min(df.Age))
print(max(df.Age))



# *** Missing Vals

df.isnull().values.any()
df.isnull().sum()
df.isnull().sum().sum()

# Housing

pd.crosstab(data.Housing,data.Default_On_Payment)
print(df.Housing.describe())
print(df.Housing.unique())
print(df.Housing.value_counts())
sns.countplot(x='Housing',data=df, palette='hls')
plt.show()
plt.savefig('count_plot')
print(df.Housing.isnull().sum())
pd.crosstab(data.Housing.isnull(),data.Default_On_Payment)

df['Housing'].mode()
df['Housing'].mode()[0]
df.Housing[df.Housing=='A152'].count()
df['Housing'].fillna(df['Housing'].mode()[0],inplace = True)
df.Housing[df.Housing=='A152'].count()
df.Housing.isnull().sum()



'''Bivariate Analysis , Missing Values ,Dummmy Variables'''


print(pd.crosstab(data.Num_Dependents, data.Default_On_Payment))
print(data.Num_Dependents.value_counts())

df2 = pd.DataFrame()

def get_Percent(col,df):
    grps =df.groupby([col,'Default_On_Payment'])
    print(grps)
    for name, group in grps:
      

        df2.loc[name[0], name[1]] = len(group)

   
    df2['Percentage 0'] = df2[0] * 100 / (df2[0] + df2[1])
    df2['Percentage 1'] = df2[1] * 100 / (df2[0] + df2[1])
    

    print(df2.sort_values(by='Percentage 1'))
    # print(df2.sort_values(by='Percentage 1'))


cols = ['Num_Dependents']
for col in cols:
    get_Percent(col, df_base)
df.shape
df = df.drop(['Customer_ID','Num_Dependents'],axis=1)
df.shape


        '''Job Status'''

df['Job_Status'].unique()
df['Job_Status'].describe()
df['Job_Status'].isnull().sum()
print(df.Job_Status.value_counts())
sns.countplot(x='Job_Status',data=df, palette='hls')
plt.show()
plt.savefig('count_plot')
df['Job_Status'].mode()
pd.crosstab(data.Job_Status,data.Default_On_Payment)

df['Job_Status'].fillna(df['Job_Status'].mode()[0],inplace = True)




     '''Adv Missing techniques'''


f1 = model_data['Job_Status']=='A171'
f2 = model_data['Job_Status']=='A172'
f3 = model_data['Job_Status']=='A173'

print(model_data.shape)

model_data['Dummy_A171'] = np.where(f1, 1, 0)
model_data['Dummy_A172'] = np.where(f2, 1, 0)
model_data['Dummy_A173'] = np.where(f3, 1, 0)

model_data = model_data.drop(['Job_Status'],axis=1)


cols =['Purpose_Credit_Taken','Status_Checking_Accnt','Credit_History','Job_Status',
       'Years_At_Present_Employment','Marital_Status_Gender',
       'Other_Debtors_Guarantors','Housing','Foreign_Worker']
for col in cols:
    get_Percent(col,df_base)

print(df['Purpose_Credit_Taken'].value_counts())
#get_Percent('Purpose_Credit_Taken',df_base)

df2 = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})

df2['color'] = np.where(df2['Set']=='Z', 'green', 'red')

model_data2 = pd.DataFrame({'Type':list('ABBCDD'), 'Set':list('ZZXYWW')})
print(model_data2)




f1 = model_data['Purpose_Credit_Taken']=='P41'
f2 = model_data['Purpose_Credit_Taken']=='P43'
f3 = model_data['Purpose_Credit_Taken']=='P48'

model_data['Dummy_Purpose_Credit_Taken_Low'] = np.where(np.logical_or(f1,np.logical_or(f2,f3)), 1, 0)

data_sbst1 = model_data[['Purpose_Credit_Taken','Dummy_Purpose_Credit_Taken_Low']]
print(data_sbst1.head())

f1 = model_data['Purpose_Credit_Taken']=='P49'
f2 = model_data['Purpose_Credit_Taken']=='P40'
f3 = model_data['Purpose_Credit_Taken']=='P45'
f4 = model_data['Purpose_Credit_Taken']=='P50'
f5 = model_data['Purpose_Credit_Taken']=='P46'
#f3 = model_data['Purpose_Credit_Taken']=='P48'
print(og_data.shape)
model_data['Dummy_Purpose_Credit_Taken_High'] = np.where(np.logical_or(f1,np.logical_or(f2,np.logical_or(f3,np.logical_or(f4,f5)))), 1, 0)
data_sbst1 = model_data[['Purpose_Credit_Taken','Dummy_Purpose_Credit_Taken_Low','Dummy_Purpose_Credit_Taken_High']]



 # '''Job Status'''

df['Job_Status'].unique()
df['Job_Status'].describe()
df['Job_Status'].isnull().sum()
print(df.Job_Status.value_counts())
sns.countplot(x='Job_Status',data=df, palette='hls')
plt.show()
plt.savefig('count_plot')
df['Job_Status'].mode()
pd.crosstab(data.Job_Status,data.Default_On_Payment)

df['Job_Status'].fillna(df['Job_Status'].mode()[0],inplace = True)




     #'''Adv Missing techniques'''
model_data['Dummy_Purpose_Credit_Taken_High'] = np.where(np.logical_or(f1,np.logical_or(f2,np.logical_or(f3,np.logical_or(f4,f5)))), 1, 0)
data_sbst1 = model_data[['Purpose_Credit_Taken','Dummy_Purpose_Credit_Taken_Low','Dummy_Purpose_Credit_Taken_High']]



ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
#We'll divide the ages into bins such as 18-25, 26-35,36-60 and 60 and above.
#Understand the output - '(' means the value is included in the bin, '[' means the value is excluded
bins = [18, 25, 35, 60, 100]
cats = pd.cut(ages, bins)
cats

#Categories (4, object): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]type(cats)
cats[0]
#To include the right bin value, we can do:
pd.cut(ages,bins,right=False)
#Categories (4, object): [[18, 25) < [25, 35) < [35, 60) < [60, 100)]

bins = [0,30,50,100]
ages = model_data.Age
cut =pd.cut(ages,bins,right=True)
pd.crosstab(data.Age,data.Default_On_Payment)
pd.crosstab(data.Age,data.Default_On_Payment).plot(kind='bar')
plt.title(' Frequency of Defaulters')
plt.xlabel('Age')
plt.ylabel('Frequency of Defaults')
plt.savefig('dflt_fre_job')
plt.show()
#bins = [0,30,50,100]#ages = model_data.Age
#pd.cut(ages,bins,right=True)
#df.loc[df['Age']>75,'Age'] = 75
ages = pd.DataFrame([81, 42, 18, 55, 23, 35], columns=['age'])
bins = [18, 30, 40, 50, 60, 70, 120]
labels = ['18-29', '30-39', '40-49', '50-59', '60-69', '70+']
ages['agerange'] = pd.cut(ages.age, bins, labels = labels,include_lowest = True)
print(ages)

print(model_data.shape)
model_data[['Age']].head()
model_data_bkp2 = model_data
bins = [0,30,100]
ages = model_data.Age
lbls = [1,0]
model_data['Dummy_Age_Group'] = pd.cut(ages,labels=lbls,bins=bins)#,right=True)
#pd.cut(ages,[1,0],[0-30,30-100])
print(model_data.shape)
model_data[['Age','Dummy_Age_Group']].head()
model_data[['Age','Dummy_Age_Group']].sample(15)
model_data.shape
model_data = model_data.drop(['Age'],axis=1)
model_data.shape



X = model_data.ix[:,(1,2,8,10,13,14,15,16,17)] # ivs for train
X
y = model_data.ix[:,14]
y


#MOdel_Creation

import statsmodels.api as sm

df2 = model_data._get_numeric_data() #drop non-numeric cols
df2.head()
X = df2.loc[:, df2.columns!='Default_On_Payment'].values
#df.drop('b', axis=1)
X
#y = dataset.iloc[:, 4].values
y = df2.iloc[:, 5].values
y

logit = sm.Logit( y, sm.add_constant(X) )
lg = logit.fit()
lg.summary()


def get_significant_vars( lm ):
    var_p_vals_df = pd.DataFrame( lm.pvalues )
    print(var_p_vals_df)
    var_p_vals_df['vars'] = var_p_vals_df.index
    print(var_p_vals_df)
    var_p_vals_df.columns = ['pvals', 'vars']
    print(var_p_vals_df)
    return list( var_p_vals_df[var_p_vals_df.pvals <= 0.05]['vars'] )

significant_vars = get_significant_vars( lg )
significant_vars

X = df2.ix[:,(0,2,4,9)].values
X
logit = sm.Logit( y, sm.add_constant(X) )
lg = logit.fit()
lg.summary()

significant_vars = get_significant_vars( lg )
significant_vars

X = df2.ix[:,(0,1,4)].values
X
logit = sm.Logit( y, sm.add_constant(X) )
lg = logit.fit()
lg.summary()
significant_vars = get_significant_vars( lg )
significant_vars


X = df2.ix[:,(0,2)].values
X
logit = sm.Logit( y, sm.add_constant(X) )
lg = logit.fit()
lg.summary()
significant_vars = get_significant_vars( lg )
significant_vars

X = df2.ix[:,(0,1)].values
X
logit = sm.Logit( y, sm.add_constant(X) )
lg = logit.fit()
lg.summary()
significant_vars = get_significant_vars( lg )
significant_vars

regr = LogisticRegression()
regr.fit(X, y)
print(regr.coef_)
pred = regr.predict(X)
pred

cm_df = pd.DataFrame(confusion_matrix(y, pred).T, index=regr.classes_,columns=regr.classes_)
cm_df.index.name = 'Predicted'
cm_df.columns.name = 'True'
cm_df

                    #The_END

