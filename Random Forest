Random Forest-Random Forest is an Ensemble of decision trees. It gives better prediction and accuracy than decision tree



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('/home/azad/Data_Science_Courses/01_DataScience-20190528T065230Z-001/01_DataScience/Datasets/diabetes2.csv')

array = data.values
array
type(array)
X = array[:,0:8] # ivs for train
X
y = array[:,8] # dv
y

test_size = 0.33
from sklearn.model_selection import train_test_split
#pip install -U scikit-learn
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=test_size)
print('Partitioning Done!')




from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
model = RandomForestClassifier(max_depth=5,n_estimators=500,oob_score=True,max_features='log2')#DecisionTreeClassifier()
model.fit(X_train,y_train)
prediction = model.predict(X_test)
outcome = y_test
print(metrics.accuracy_score(outcome,prediction))
print(metrics.confusion_matrix(y_test,prediction)) #
importance =model.feature_importances_


indices = np.argsort(importance)[::-1]
feature = data[data.columns[0:8]]
feat_names = data.columns[0:8]
print("DecisionTree Feature ranking:")
for f in range(feature.shape[1]):
    print("%d. feature %s (%f)" % (f + 1, feat_names[indices[f]], importance[indices[f]]))
plt.figure(figsize=(15,5))
plt.title("DecisionTree Feature importances")
plt.bar(range(feature.shape[1]), importance[indices], color="y", align="center")
plt.xticks(range(feature.shape[1]), feat_names[indices])
plt.xlim([-1, feature.shape[1]])
plt.show()
print(metrics.accuracy_score(outcome,prediction))
